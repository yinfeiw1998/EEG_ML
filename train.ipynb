{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import features\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the file that contains feature data and read it\n",
    "dataX1 = []\n",
    "dataX2 = []\n",
    "dataX3 = []\n",
    "dataX4 = []\n",
    "dataX5 = []\n",
    "dataX6 = []\n",
    "dataX = []\n",
    "\n",
    "dataY = []\n",
    "file_name1 = \"features(without ica)\"\n",
    "file_name2 = \"features(with ica).txt\"\n",
    "f = open(file_name1, \"r\")\n",
    "#The 0-3000 belongs to 101\n",
    "#The 3000-6000 belongs to 202\n",
    "count = 1\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "    if(line == \"id | rms | ApprEnt | LZComp | mpf | sef\\n\"):\n",
    "        #print(\"found\")\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            raw_data = [i.strip().split() for i in line.strip().split('|')]\n",
    "#             dataX1.append([float(i[0]) for i in raw_data[1:]])\n",
    "#             dataX2.append([float(i[1]) for i in raw_data[1:]])\n",
    "#             dataX3.append([float(i[2]) for i in raw_data[1:]])\n",
    "#             dataX4.append([float(i[3]) for i in raw_data[1:]])\n",
    "#             dataX5.append([float(i[4]) for i in raw_data[1:]])\n",
    "#             dataX6.append([float(i[5]) for i in raw_data[1:]])\n",
    "            temp = []\n",
    "            for i in raw_data[1:]:\n",
    "                for j in i:\n",
    "                    temp.append(float(j))\n",
    "            dataX.append(temp)\n",
    "            dataY.append(int(raw_data[0][0]))\n",
    "#             if (count >3000 and count<=6000):\n",
    "#                 dataY.append(202)\n",
    "#             elif (count > 0 and count<=3000):\n",
    "#                 dataY.append(101)\n",
    "#             else:\n",
    "#                 dataY.append(int(raw_data[0][0]))\n",
    "#             count += 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "        #print(dataX[1])\n",
    "        \n",
    "        #for word in line.split(' | '):\n",
    "            #try:\n",
    "            #    row.append(float(word))\n",
    "            #except ValueError:\n",
    "            #    pass\n",
    "\n",
    "        #print(row)\n",
    "    \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20384\n",
      "20384\n"
     ]
    }
   ],
   "source": [
    "print(len(dataY))\n",
    "print(len(dataX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='kd_tree', leaf_size=7, n_neighbors=6,\n",
      "                     weights='distance')\n",
      "0.6193279372087319\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC())])\n",
      "0.7022320333578612\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=10, max_leaf_nodes=340,\n",
      "                       random_state=1)\n",
      "0.6198184939906795\n",
      "RandomForestClassifier(max_depth=22, n_estimators=285, random_state=1)\n",
      "0.8273240127544763\n",
      "MLPClassifier(activation='logistic', alpha=1e-05, hidden_layer_sizes=(200, 75),\n",
      "              random_state=1)\n",
      "0.6271768457198921\n",
      "AdaBoostClassifier(algorithm='SAMME', learning_rate=1, n_estimators=100,\n",
      "                   random_state=0)\n",
      "0.36276674025018396\n",
      "GaussianNB()\n",
      "0.31788079470198677\n",
      "VotingClassifier(estimators=[('SVM',\n",
      "                              Pipeline(steps=[('standardscaler',\n",
      "                                               StandardScaler()),\n",
      "                                              ('svc', SVC())])),\n",
      "                             ('RandomForest',\n",
      "                              RandomForestClassifier(max_depth=22,\n",
      "                                                     n_estimators=285,\n",
      "                                                     random_state=1))])\n",
      "0.7571743929359823\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(dataX, dataY, test_size=0.20, random_state=42)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "\n",
    "#initialize classifier \n",
    "#for 10 people\n",
    "#nneighbor is 7, weight is distance and score is 76.32\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=6,weights = 'distance', algorithm = 'kd_tree', leaf_size = 7)\n",
    "# score: 84.08\n",
    "clf_SVM = make_pipeline(StandardScaler(), SVC(gamma='scale', kernel='rbf'))\n",
    "# best score: 78.63\n",
    "clf_DT = DecisionTreeClassifier(random_state=1,criterion='entropy', splitter='best', max_depth=10,max_features=None, max_leaf_nodes=340)\n",
    "# best score: 92.33\n",
    "clf_RF = RandomForestClassifier(max_depth=22, random_state=1, n_estimators = 285)\n",
    "# best score is: 75.2\n",
    "clf_NN = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(200,75), random_state=1, activation='logistic')\n",
    "# score: 65\n",
    "clf_AB = AdaBoostClassifier(n_estimators=100, random_state=0, learning_rate=1, algorithm='SAMME')\n",
    "# clf_AB.fit(X_train, Y_train)\n",
    "# print(clf_AB.score(X_test, Y_test))\n",
    "#score: 49\n",
    "clf_gnb = GaussianNB()\n",
    "# clf_gnb.fit(X_train, Y_train)\n",
    "# print(clf_gnb.score(X_test, Y_test))\n",
    "# clf_QDA = QDA()\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "eclf = VotingClassifier(estimators=[ ('SVM', clf_SVM), ('RandomForest', clf_RF)], \n",
    "                                    voting='hard')\n",
    "                                    \n",
    "# eclf.fit(X_train, Y_train)\n",
    "# print(eclf.score(X_test, Y_test))\n",
    "#100 110\n",
    "# for i in range(50, 200, 5):\n",
    "#     print(i)\n",
    "#     clf_AB = AdaBoostClassifier(n_estimators=i, random_state=0, learning_rate=1, algorithm='SAMME')\n",
    "#     clf_AB.fit(X_train, Y_train)\n",
    "#     print(clf_AB.score(X_test, Y_test))\n",
    "\n",
    "clf_ensem = [clf_KNN, clf_SVM, clf_DT, clf_RF, clf_NN, clf_AB, clf_gnb, eclf]\n",
    "for clf in clf_ensem:\n",
    "    print(clf)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print(clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataX[0])\n",
    "len(dataX1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN classifier, dataX3 got the highest score 80.333%\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(dataX, dataY, test_size=0.20, random_state=42)\n",
    "#when neighbor is 11, the score is the highest:0.741\n",
    "# for i in range(5,40):\n",
    "#     clf_KNN = KNeighborsClassifier(n_neighbors=i)\n",
    "#     clf_KNN.fit(X2_train, Y2_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8033333333333333\n"
     ]
    }
   ],
   "source": [
    "clf_KNN = KNeighborsClassifier(n_neighbors=11,weights = 'uniform')\n",
    "clf_KNN.fit(X2_train, Y2_train)\n",
    "#pr = clf.predict_proba(X1_test)\n",
    "print(clf_KNN.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,40):\n",
    "    clf_Knn = KNeighborsClassifier(n_neighbors=i, weights = 'uniform', algorithm = 'auto')\n",
    "    clf_KNN.fit(X2_train, Y2_train)\n",
    "    #print(clf.score(X1_test, Y1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# SVM classifier   dataX5 get the highest score 95.56%\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "clf_SVM = make_pipeline(StandardScaler(), SVC(gamma='scale', kernel='rbf'))\n",
    "clf_SVM.fit(X2_train, Y2_train)\n",
    "print(clf_SVM.score(X2_test, Y2_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Process classifier \n",
    "# well, this classifier take too much time and it did not give a good score ---about 72%?\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "kernel = 1.0 * RBF(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_GPC = GaussianProcessClassifier(kernel=kernel,random_state=0,n_jobs = -1).fit(X2_train[0:700], Y2_train[0:700])\n",
    "# print(cls_GPC.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9122222222222223\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree classifier\n",
    "# The best score got 91.23% for datachannel5\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(dataX, dataY, test_size=0.20, random_state=42)\n",
    "\n",
    "clf_DT = DecisionTreeClassifier(random_state=0,criterion='entropy', splitter='best', max_depth=7,max_features=None,min_samples_split=15,min_samples_leaf=5, max_leaf_nodes=75)\n",
    "clf_DT.fit(X2_train, Y2_train)\n",
    "print(clf_DT.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,50):\n",
    "    clf_DT = DecisionTreeClassifier(random_state=0,criterion='entropy', splitter='best', max_depth=7,min_samples_split=15,min_samples_leaf=5,max_leaf_nodes=75, min_impurity_decrease=i)\n",
    "    clf_DT.fit(X2_train, Y2_train)\n",
    "#     print(clf_DT.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8311111111111111\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "#The best score is 83.11%\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(dataX, dataY, test_size=0.20, random_state=42)\n",
    "clf_RF = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf_RF.fit(X2_train, Y2_train)\n",
    "print(clf_RF.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7966666666666666\n"
     ]
    }
   ],
   "source": [
    "#Nueral Network model: Multilayer perceptron\n",
    "#best score for NN => 79.6%\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(dataX, dataY, test_size=0.20, random_state=42)\n",
    "clf_NN = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(10, 2), random_state=1, activation='tanh')\n",
    "clf_NN.fit(X2_train, Y2_train)\n",
    "print(clf_NN.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### for i in range(3,20):\n",
    "    clf_NN = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(20, 2), random_state=1, activation='tanh')\n",
    "    clf_NN.fit(X2_train, Y2_train)\n",
    "    print(clf_NN.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost Classifier \n",
    "#best score 92.6\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf_AB = AdaBoostClassifier(n_estimators=100, random_state=0, learning_rate=1, algorithm='SAMME.R')\n",
    "clf_AB.fit(X2_train, Y2_train)\n",
    "print(clf_AB.score(X2_test, Y2_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6855555555555556\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "# best score 68.5%\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_gnb = GaussianNB(var_smoothing = 1e-9)\n",
    "clf_gnb.fit(X2_train, Y2_train)\n",
    "print(clf_gnb.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8488888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinfeiwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "#QDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "clf_QDA = QDA()\n",
    "clf_QDA.fit(X2_train, Y2_train)\n",
    "print(clf_QDA.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingClassifier\n",
    "# eclf = VotingClassifier(estimators=[ ('SVM', clf_SVM), ('DecisionTree', clf_DT), ('RandomForest', clf_RF), \n",
    "#                                    ('AdaBoost', clf_AB), ('QDA', clf_QDA)], \n",
    "#                                     voting='soft', weights=[95.5, 91.2, 83.1,92.6,84.8])\n",
    "# eclf.fit(X2_train, Y2_train)\n",
    "# print(eclf.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinfeiwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9588888888888889\n"
     ]
    }
   ],
   "source": [
    "#For this voting clf: best score is 95.8\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "eclf = VotingClassifier(estimators=[ ('SVM', clf_SVM), ('DecisionTree', clf_DT), ('RandomForest', clf_RF), \n",
    "                                   ('AdaBoost', clf_AB), ('QDA', clf_QDA)], \n",
    "                                    voting='hard')\n",
    "eclf.fit(X2_train, Y2_train)\n",
    "print(eclf.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#For this one the best score is 96.67%\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "eclf = VotingClassifier(estimators=[ ('SVM', clf_SVM), ('DecisionTree', clf_DT), \n",
    "                                   ('AdaBoost', clf_AB)], \n",
    "                                    voting='hard')\n",
    "eclf.fit(X2_train, Y2_train)\n",
    "print(eclf.score(X2_test, Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
